{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4dbee3",
   "metadata": {},
   "source": [
    "# Source\n",
    "https://github.com/lucidrains/musiclm-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7caaa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
    "\n",
    "audio_transformer = AudioSpectrogramTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    spec_n_fft = 128,\n",
    "    spec_win_length = 24,\n",
    "    spec_aug_stretch_factor = 0.8\n",
    ")\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "mulan = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")\n",
    "\n",
    "# get a ton of <sound, text> pairs and train\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "texts = torch.randint(0, 20000, (2, 256))\n",
    "\n",
    "loss = mulan(wavs, texts)\n",
    "loss.backward()\n",
    "\n",
    "# after much training, you can embed sounds and text into a joint embedding space\n",
    "# for conditioning the audio LM\n",
    "\n",
    "embeds = mulan.get_audio_latents(wavs)  # during training\n",
    "\n",
    "embeds = mulan.get_text_latents(texts)  # during inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56914e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiclm_pytorch import MuLaNEmbedQuantizer\n",
    "\n",
    "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
    "\n",
    "quantizer = MuLaNEmbedQuantizer(\n",
    "    mulan = mulan,                          # pass in trained mulan from above\n",
    "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
    "    namespaces = ('semantic', 'coarse', 'fine')\n",
    ")\n",
    "\n",
    "# now say you want the conditioning embeddings for semantic transformer\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88be06f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'checkpoint_path' and 'kmeans_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n\u001b[0;32m----> 4\u001b[0m wav2vec \u001b[38;5;241m=\u001b[39m HubertWithKmeans(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#checkpoint_path = './hubert/hubert_base_ls960.pt',\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m semantic_transformer \u001b[38;5;241m=\u001b[39m SemanticTransformer(\n\u001b[1;32m     10\u001b[0m     num_semantic_tokens \u001b[38;5;241m=\u001b[39m wav2vec\u001b[38;5;241m.\u001b[39mcodebook_size,\n\u001b[1;32m     11\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     12\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     13\u001b[0m     audio_text_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m      \u001b[38;5;66;03m# this must be set to True (same for CoarseTransformer and FineTransformers)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SemanticTransformerTrainer(\n\u001b[1;32m     17\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m semantic_transformer,\n\u001b[1;32m     18\u001b[0m     wav2vec \u001b[38;5;241m=\u001b[39m wav2vec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     num_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'checkpoint_path' and 'kmeans_path'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
    "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
    ").cuda()\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
    "    folder ='/path/to/audio/files',\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need the trained AudioLM (audio_lm) from above\n",
    "# with the MulanEmbedQuantizer (mulan_embed_quantizer)\n",
    "\n",
    "from musiclm_pytorch import MusicLM\n",
    "\n",
    "musiclm = MusicLM(\n",
    "    audio_lm = audio_lm,\n",
    "    mulan_embed_quantizer = mulan_embed_quantizer\n",
    ")\n",
    "\n",
    "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 4) # sample 4 and pick the top match with mulan\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai-cinema-engine-venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
